{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adc7b69",
   "metadata": {},
   "source": [
    "## Data Manipulation with pandas\n",
    "\n",
    "### Reading and writing csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe= pd.read_csv(\"file_name.csv\")      #read csv file\n",
    "dataframe.to_csv(\"new_file_name.csv\")        #save dataframe to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d0be4",
   "metadata": {},
   "source": [
    "### Data viewing/Data summary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(n)                     #returns first n rows of dataframe\n",
    "dataframe.info()                      #given info like number of rows and columns,null count,datatype, memory usage\n",
    "dataframe.shape                       #(rows count,columns count)\n",
    "dataframe.describe()                  #statistical data for numerical dtype columns like count, min,max,mean,etc.\n",
    "dataframe.values                      #shows data in 2D array\n",
    "dataframe.index                       #row number, row names\n",
    "dataframe.columns                     #returns column names and dtype\n",
    "dataframe.column_name.value_counts()  #count of unique values in a column\n",
    "dataframe['column_name'].value_counts()\n",
    "dataframe['clmn'].isnull().sum        #counts number of null rows in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b28f44",
   "metadata": {},
   "source": [
    "### Sorting and Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"column_name\")                                            #sorting in ascending order\n",
    "df.sort_values(\"column_name\",ascending=False)                            #sorting in descendng order\n",
    "df.sort_values([\"c1,\",\"c2\"],ascending=[True,False])                      #sorting by multiple variables\n",
    "df['column']                                                             #column subsetting\n",
    "df['c1','c2']\n",
    "df['column'] > n                                                         #rows subsetting\n",
    "df[df['column']>n]                                                       #returns all columns with filter\n",
    "df[df['column']=='text']                                                 #subsetting based on text data\n",
    "df[df['date_column']<\"YYYY-MM-DD\"]                                       #subsetting date column\n",
    "\n",
    "#subsetting based on multiple conditions\n",
    "condition1 =df[\"clmn1\"]==\"condition\"\n",
    "condition2 =df[\"clmn2\"]==\"condition\"\n",
    "df[condition1 & condition2]\n",
    "\n",
    "#subsetting using .isin()\n",
    "c1_or_c2 = df[\"clmn\"].isin(['c1','c2'])\n",
    "df[c1_or_c2]\n",
    "\n",
    "#new column\n",
    "df['height_m']= df['height_cm']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2fbea4",
   "metadata": {},
   "source": [
    "### Statistics using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic statistical functions\n",
    "df['clmn'].mean()\n",
    "df['clmn'].mean(axis='index')                                #across rows\n",
    "df['clmn'].mean(axis='columns')                              #across columns\n",
    "df['clmn'].median()\n",
    "df['clmn'].mode()\n",
    "df['clmn'].min()\n",
    "df['clmn'].max()\n",
    "df['clmn'].std()\n",
    "df['clmn'].var()\n",
    "df['clmn'].sum()\n",
    "df['clmn'].quantile()\n",
    "\n",
    "#agg() is used to calculate data in a series, used to pass multiple functions at once\n",
    "df['column'].agg(['mean','sum'])                              #single column\n",
    "df.agg({'column1':['mean','sum'],'column2':'mean'})           #multiple columns\n",
    "\n",
    "#cummulative functions\n",
    "df['column'].cumsum()\n",
    "df['column'].cumprod()\n",
    "df['column'].cummin()\n",
    "df['column'].cummax()\n",
    "\n",
    "#grouped summary statistics\n",
    "df.groupby('column1')['column2'].agg(['mean','sum'])          #summaries of column2 grouped by column1\n",
    "df.groupby('column1')['column2'].mean()\n",
    "df.groupby(['column1','column2'])['column3'].mean()           #group by multiple variables\n",
    "\n",
    "#counting\n",
    "df.drop_duplicates(subset ='column1')                         #drop duplicates based on names\n",
    "unique=df.drop_duplicates(subset=['c1','c2'])                 #drop duplicate pairs\n",
    "unique['c1'].value_counts(normalize=True)                     #in terms of proportion of total\n",
    "unique['c1'].value_counts(sort=True)                          #sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8b50b",
   "metadata": {},
   "source": [
    "### Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by using pivot tables:\n",
    "df.groupby(\"c1\")[\"c2\"].mean() \n",
    "#can be written as ->\n",
    "df.pivot_table(values=\"c2\",index=\"c1\")\n",
    "\n",
    "#by default pivot table returns mean value. For other statistical values:\n",
    "df.pivot_table(values=\"c2\",index=\"c1\",aggfunc=[np.mean,np.median])\n",
    "\n",
    "#pivot on 2 variables\n",
    "df.groupby([\"c1\",\"c2\"])[\"c3\"].mean()\n",
    "#can be written as ->\n",
    "df.pivot_table(values=\"c3\",index=\"c1\",columns=\"c2\",fill_value=0, margins=True)\n",
    "#fill_value fills missing values\n",
    "#margins argument returns mean of each row and column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c4842",
   "metadata": {},
   "source": [
    "### Explicit Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index= df.set_index(\"column\")                                            #setting the column as index,we can index any column\n",
    "df_index.reset_index()                                                      #removing an index\n",
    "df_index.reset_index(drop=True)                                             #entirely removes the index\n",
    "\n",
    "df_ind2= df.set_index([\"column1\",\"column2\"])                                #multi level indexes\n",
    "df_ind2= df.set_index(level=[\"column1\",\"column2\"],ascending=[True,False])   #controlling sort index\n",
    "df_ind2.loc[[\"vc1\",\"vc2\"]]                                                  #subset with a list, vc1:column1::vc2:column2\n",
    "df_ind2.loc[[(\"vc11\",\"vc21\"):(\"vc12\",\"vc22\")]]                              #subset inner levels with a list of tuples\n",
    "\n",
    "#Indexes makes subsetting easier\n",
    "df[df[\"column\"]].isin([\"v1\",\"v2\"])\n",
    "#can be coded as ->\n",
    "df_index.loc[[\"v1\",\"v2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df69e2a",
   "metadata": {},
   "source": [
    "### Slicing/Subsetting using .loc[] and .iloc[]\n",
    "slicing based on row and column label:   **.loc[row_label, column_label]**<br>\n",
    "slicing based on indexes:   **.iloc[row_index,column_index]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index([\"c1\",\"c2\"]).sort_index()                            #first sort the dataset before slicing it\n",
    "df.loc[(\"vc11\",\"vc21\"):(\"vc12\",\"vc22\")]                              #using tuples slicing at inner level\n",
    "df.loc[(\"vc11\",\"vc21\"):(\"vc12\",\"vc22\"),\"column3\":\"column4\"]          #slicing rows and columns\n",
    "df.loc[\"2023-09-09\":\"2023-12-18\"]                                    #slicing by dates\n",
    "\n",
    "#iloc\n",
    "df.iloc[2:5,1:4]                                                     # 5 and 4 are not inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6838d",
   "metadata": {},
   "source": [
    "### Handeling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e52e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()                                      #detect missing values\n",
    "df.isna().any()                                #detect missing value(if any) in a column\n",
    "df.isna().sum()                                #counting missing values\n",
    "df.dropna()                                    #remove rows with missing values\n",
    "df.fillna(value)                               #replace missing values with another value\n",
    "\n",
    "import mtplotlib.pyplot as plt\n",
    "df.isna().sum.plot(kind=\"bar\")                 #plot missing data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df11c4",
   "metadata": {},
   "source": [
    "### Dataframe creation\n",
    "<img src=\"Df.JPG\" />\n",
    "\n",
    "There are 2 ways to create dataframe:<br>\n",
    " __List of Dictionaries__: create data row by row<br>\n",
    " __Dictionaries of lists__ : create data column wise<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04211aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of dictionaries:\n",
    "df= [{\"name\":'Ginger','Breed':'Labrador','Height':22,'DOB':'2019-03-14'}\n",
    "     {'name':'Scout','Breed':'Dalmatian','Height':59,'DOB':'2019-05-02'}\n",
    "]\n",
    "dogs=pd.DataFrame(df)\n",
    "\n",
    "#dictionaries of lists:\n",
    "df= {\n",
    "    \"name\":['Ginger','Scout'],\n",
    "    \"Breed\":[\"Labrador\",\"Dalmatian\"],\n",
    "    \"Height\":[22,59],\n",
    "    \"DOB\":[\"2019-03-14\",\"2019-05-02\"]\n",
    "}\n",
    "dogs=pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c92e4",
   "metadata": {},
   "source": [
    "## Reshaping Data with Pandas\n",
    "\n",
    "Data formatting is requires if data is not inappropriate format for analysis<br>\n",
    "**Wide Format** : columns> rows, used for simple stats and manipulation<br>\n",
    "                  No repitation but large number of missing data<br>\n",
    "**Long Format** : rows>columns; tidy data; preffered for analysis and graphing<br>\n",
    "**Wide to Long Format** : .melt() and .wide_to_long()<br>\n",
    "**Long to wide Format** : .pivot() and .pivot_table()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00626b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape()                                      #necessary to know shape of data\n",
    "df.set_index(\"c1\")[[\"c2\",\"c3\"]].transpose()     #transpose() is used to exchange indexes and columns\n",
    "\n",
    "#long to wide reshaping: operations that requires column to be unique variable\n",
    "#pivot method:\n",
    "#here c1- index column, c2- header column, c3- values column\n",
    "df.pivot(index=\"c1\",columns=\"c2\",values=\"c3\")\n",
    "\n",
    "#pivot table: summarizes data of a larger data frame\n",
    "#sorts limitations of pivot method\n",
    "df.pivot_table(index=\"c1\",columns=\"c2\",values=\"c3\",aggfunc=['mean','median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a99e56",
   "metadata": {},
   "source": [
    "wide to long reshaping using .melt()\n",
    "<img src=\"melt.JPG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c11172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using .melt()\n",
    "#id_vars:identifier variables, value_vars:columns we want to melt,\n",
    "#var_name: name of variable column , value_name: name of value column\n",
    "df.melt(id_vars=['Name'],value_vars=['Math','Physics'],var_name=['Subject'],value_name=['Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4388cce2",
   "metadata": {},
   "source": [
    "wide to long reshaping using wide_to_long()\n",
    "<img src=\"reshape.JPG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wide to long:\n",
    "pd.wide_to_long(df,stub_names=[\"age\",\"weight\",i=\"name\",j=\"year\"])               #i = index of long dataframe\n",
    "#if i and j are unidentified by pandas an empty dataframe is returned\n",
    "pd.wide_to_long(df,stub_names=[\"age\",\"weight\",i=\"name\",j=\"year\",sep= \"-\"])      #if separator is there lke age-2019,weight-2019\n",
    "pd.wide_to_long(df,stub_names=[\"age\",\"weight\",i=\"name\",j=\"year\",suffix='\\w+'])  #if column names ends with a word not nummber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c44921",
   "metadata": {},
   "source": [
    "## String Column Operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c1'].str.split(\":\")                                      #split about delimiter :\n",
    "df['c1'].str.split(\":\").str.get(0)                           #get element of 0 index\n",
    "df[['c2','c3']]= df['c1'].str.split(\":\",expand=True)         #With 2 separate columns\n",
    "df.drop('c1',inplace=True)\n",
    "\n",
    "df['c3']=df['c1'].str.cat(df['c2'],sep=\" \")                  #concatenate two columns, assign it to new column c3\n",
    "df.index=df.index.str.cat(df['c1'],sep=\" \")                  #concatenate with index\n",
    "df.index= df.index.str.split(\"-\",expand=True)                #split index\n",
    "\n",
    "new_list=['a','b','c']\n",
    "df['c1'].str.cat(new_list,sep=\" \")                           #concatenate series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a2d4f",
   "metadata": {},
   "source": [
    "## Stacking Dataframes\n",
    "\n",
    "Another method used to pivot dataframe from wide to long format.<br>\n",
    "Level of columns are rearranged to obtain a reshaped Dataframe with a new inner-most level row index.<br>\n",
    "<img src=\"stack.JPG\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218854d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multi-level index and columns for data frame:\n",
    "index= pd.MultiIndex.from_arrays([['Wick','Wick','Shelly','Shelly'],['John','Bryan','Mary','Frank']],names=['Last','First'])\n",
    "data={'Height':[182,179,152,180],'Weight':[75,84,52,76]}\n",
    "Agents=pd.DataFrame(data,index=index)\n",
    "print(Agents)\n",
    "Agents_stacked=Agents.stack()                                #stack data!\n",
    "Agents.stack(level=0)                                        #stack level  by number\n",
    "Agents.stack(level=Agents.index.names.index('Last'))         #stack level by name\n",
    "\n",
    "#unstack() is reverse of stack\n",
    "Agents_stacked.unstack()                                     \n",
    "Agents_stacked.unstack(level=1)\n",
    "print(Agents_stacked.unstack(level='First'))\n",
    "print(Agents_stacked.unstack(level=1).stack(level=0))        #rearranging levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843455b0-e7d9-4a8f-9092-dbb58abf3534",
   "metadata": {},
   "source": [
    "### Working with multiple levels:\n",
    "Rearranging multiple levels: <br>\n",
    "- swap levels\n",
    "- stack and unstack multiple levels at the same time\n",
    "\n",
    "<img src=\"swap levels.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12b311-f2fe-41f9-bba9-c68b8e45fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.swaplevel(0,2)                                    #swap levels\n",
    "df.swaplevel(0,2).unstack()         \n",
    "df.swaplevel(0,1,axis=1)\n",
    "df.stack().swaplevel(0,2)                            #swap levels and stack\n",
    "\n",
    "df.unstack(level=[0,1])                              #unstack by number\n",
    "df.unstack(level=['clmn1','clmn2'])                  #unstack by name\n",
    "\n",
    "df.unstack.stack(level=[0,1])                        #unstack then stack by number\n",
    "df.unstack.stack(level=['clmn1','clmn2']             #unstack then stack by name\n",
    "\n",
    "df.unstack(level='clmn1',fill_value='Nan')           #handeling Nan with unstack\n",
    "df.stack(dropna=False).fillna(0)                     #handeling Nan with stack               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816529b8-607e-496b-9102-c2ebf73a4acf",
   "metadata": {},
   "source": [
    "### Reshaping and combining data\n",
    "statistical functions can be preffered on dataframe - .sum(), .mean(), .median() .diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ecd484-cb8d-434d-845b-38eba46117c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.stack().sum(axis=1)  #can also be written as\n",
    "sales.stack().sum(axis=1).unstack()\n",
    "\n",
    "sales.unstack(level=0).mean(axis=1)                                        #unstack and stats\n",
    "sales['column'].unstack(level='clmn2').diff(axis=1,periods=2)              #difference in amount of sales b/w years\n",
    "\n",
    "sales.stack().groupby(level='shop').sum()                                  #reshaping and grouping\n",
    "sales.groupby(level='year').median()                                       #reshaping after grouping\n",
    "\n",
    "sales.groupby(level=1).median().stack(level=[0,1]).unstack(level='year')   #median amnt of products by year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bc237-f380-4a6e-b494-064a313048f8",
   "metadata": {},
   "source": [
    "### Transforming list-like columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffed43-6da2-4112-8450-7f2b36d8d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explode=df['clmn2'].explode()\n",
    "df[['clmn2','clmn3']].merge(df_explode,left_index=True,right_index=True)   #back in dataframe\n",
    "\n",
    "df.assign(df1=df['clmn1'].str.split(',')).explode('clmn1')                 #chaining operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2168cf-1ca7-4186-a068-10acacd40de8",
   "metadata": {},
   "source": [
    "### Reading nested data into a dataframe\n",
    "**JSON format:**\n",
    "- JavaScrip Object Notation -Data interchange format\n",
    "- Easy to read and right ,easy for machines to process and generate\n",
    "- JSON is very similar to data library\n",
    "- record_path : tells pandas which path leads to each individuals records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8813c32-8297-4854-bd1d-0627db1aa0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data normalization:\n",
    "from pandas import json.normalize\n",
    "json_normalize(writers)\n",
    "writers_norm =json_normalize(writers,sep='-')\n",
    "\n",
    "#complex JSON\n",
    "writers_norm=json_normalise(writers,record_path='books')\n",
    "json_normalize(writers,record_path='books',meta=['name','last'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa184401-c8b2-4466-825a-5fb140227f33",
   "metadata": {},
   "source": [
    "### Dealing with nested data columns \n",
    "writers =[                    ]<br>\n",
    "books = [\" {                  }\",\"{                   }\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3a0cb-25c7-4433-912d-dd3128a6f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "colllection =pd.DataFrame(dict(writers=writers,books=books))         #nested data in columns\n",
    "\n",
    "import json\n",
    "books=collection['books'].apply(json.loads).apply(pd.Series)         #converting nested data- will convert json data into python dictionary\n",
    "\n",
    "collection =collection.drop(columns ='books')\n",
    "pd.concat([collection,books],axis=1)                                 #concat both datas\n",
    "\n",
    "import json\n",
    "books =collection['books'].apply(json.loads).to_list()\n",
    "books_dump =json.dumps(books)                                        #dumping nested data\n",
    "new_books=pd.read_json(books_dump)\n",
    "pd.concat([collection['writers'],new_books,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8746a",
   "metadata": {},
   "source": [
    "## Joining Data with Pandas\n",
    "**Consider tables with following columns :**<br>\n",
    "ward(shape: 50,4) -> [ward,alderman,address,zip]<br>\n",
    "census(shape: 50,6) -> [ward,pop_2010,pop2020,change,address,zip]<br>\n",
    "grants(shape: 50,4) -> [address,zip,grant,company]<br>\n",
    "licences(shape:50,6) -> [account,ward,aid,business,address,zip]<hr>\n",
    "movie(shape:4803,4) -> [id,original_title,popularity,release_date]<br>\n",
    "tagline(shape: 3955,2) ->[id,tagline]<br>\n",
    "tv_genre(shape: 3000,2) ->[movie_id,tv_genre]\n",
    "\n",
    "<img src='joins.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc42c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_census=ward.merge(census, on='ward')                                           #inner join\n",
    "ward_census=ward.merge(census,on='ward',suffixes=('_w','_c'))                       #control suffix\n",
    "\n",
    "#merge on multiple columns,both adress & zip must be same in both\n",
    "grants_license =grants.merge(license,on=['address,zip'])\n",
    "#merge multiple tables\n",
    "multi_merge= gets.merge(license, on=['address','zip']).merge(ward,on='ward',suffix=('_base','_w')).merge(....)\n",
    "\n",
    "movie_tagline =movie.merge(tagline, on='id',how='left')                            #left join\n",
    "movie_tv= movie.merge(tv_genre, how='left',left_on='id',right_on='movie_id')       #left join on different column names\n",
    "\n",
    "movie_tagline =movie.merge(tagline,on='id',how='outer',suffix=('_m','_t'))         #outer join;suffix to differnetiate columns\n",
    "\n",
    "sequals.merge(sequals,left_on='sequal',right_on='id',suffix=('_m','_s'))           #self join;merge here is inner join\n",
    "\n",
    "#merging on index:\n",
    "movies=pd.read_csv('movies.csv',index_col=['id'])                                  #setting index\n",
    "movies_genre=movies.merge(to_genre,left_on='id',left_index=True,right_on='movie_id',right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a077001-0368-4021-9a01-4fbeaa860e20",
   "metadata": {},
   "source": [
    "### Advanced Merging and concatinating\n",
    "Multiple joins : Combining data from two tables based on matching observations<br>\n",
    "Filtering Joins : Filter Observations based on another tables observations<br>\n",
    "Semi Joins: Semi Joins are just like inner joins,no column from right df is returned, also no duplicates<br>\n",
    "Anti Joins: Returns rows from left table which do not have any match in right table after joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7f3eb-a30f-4112-9490-47ddaa396ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semi Joins\n",
    "genre_tracks=genre.merge(top_tracks,on='genre')\n",
    "top_genre=genre.[genre['gid'].isin(genre_tracks['gid'])]\n",
    "\n",
    "#Anti Join:\n",
    "genre_tracks=genre.merge(top_tracks,on='gid',how='left',indicator=True)\n",
    "gid_list=genre_tracks.loc[genre_tracks['_merge']=='left_only','gid']\n",
    "non-top_genre=genres[genre['gid'].isin(gid_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adf12c-6d77-4467-942c-a573f92e4471",
   "metadata": {},
   "source": [
    "### Concate Dataframes together Vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93cbe0-9791-4857-a1e7-d0311e976d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([inv_jan,inv_feb,inv_mar])                                 #basic concatenation\n",
    "pd.concat([t1,t2,t3],ignore_index=True)                              #ignore index\n",
    "pd.concat([t1,t2,t3],ignore_index=False, keys=['jan','feb','mar'])   #setting labels to original table, make sure index=False\n",
    "pd.concat([t1,t2],sort=True)                                         #arranges columns in alphabetical order\n",
    "\n",
    "#concate tables with different column names:\n",
    "pd.concat([t1,t2],join= 'inner')                                     #the inner join will ignore the uncommon column in both dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee5bc4-e1ec-4214-9a85-b38f1f377b03",
   "metadata": {},
   "source": [
    "### Verify integrity of data\n",
    "**Merging issues:** unintentional one-to-one or many-to-many relations<br>\n",
    "<u>To validate merge</u> - .merge(validate= 'x') where x : ['None','one_to_one','one_to_many','many_to_one','many_to_many']<br>\n",
    "**Concat issues:** Duplicate records possibly unintentionally introduced<br>\n",
    "<u>To validate concatenation</u> - .concat(verify_integrity = False)\n",
    "(check whether new concatenated index contains duplicates , default is false.It is necessary to verify integrity to fix incorrect data and drop duplicate rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed172262-3fea-4d09-9035-6779b9b58c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge validate\n",
    "tracks.merge(specs,on='tid',validate='one_to_one')                   #!!!! merge error\n",
    "tracks.merge(specs,on='tid',validate='one_to_many')                  #works!\n",
    "\n",
    "#concate validation\n",
    "pd.concat(verify_integrity =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed8ab7-c960-4ed3-8e26-6195a37d2d92",
   "metadata": {},
   "source": [
    "### Merging Ordered and time-series data\n",
    "**why to use merge_order() ?** <br>\n",
    "- if we have ordered data/time series\n",
    "- to fill missing data by using forward or backward filling , as they are necessary for ML models\n",
    "\n",
    "**using merge_asof()** <br>\n",
    "- similar to merge_ordered() left join, similar features as merge_ordered()\n",
    "- only catch: merge on the nearest key column and not exact matches.  **Merged 'on' column must be sorted!!**\n",
    "- Closest data less than first dataframe columns is taken from right table column.To select data greater than first df corner we use direction keyword\n",
    "- **when to use?** data sampled from a process developing a training set\n",
    "- direction keywords : 'forward', 'backward','nearest'\n",
    "\n",
    "<img src='merge.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48555ef0-146e-4575-8073-a174b96dc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using merge_ordered()\n",
    "pd.merge_ordered(appl,mcd,on='date',suffixes=('_appl','_mcd')\n",
    "pd.merge_ordered(applmmcd,on='date',suffixes=('_appl','_mcd'),fill_method='ffill')     #forward fill\n",
    "\n",
    "#using merge_asof()\n",
    "pd.merge_asof(visa,ibm,on=['data_time'],suffixes=('_v','_ibm'),direction='forward')                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1ef9-ecc7-4046-8d81-ab065dcd1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data with .query -- similar to WHERE clause in SQL statement\n",
    "stocks.query('mike >= 90')\n",
    "stocks.query('mike >90 and disney <=140')\n",
    "stocks.query('stocks == \"disney\" or (stock == \"mike\" and close<90)')\n",
    "\n",
    "#reshaping data with .melt()\n",
    "df2=df1.melt(id_vars=['c1','c2'],value_vars=['v1','v2'],var_name =['year'],value_name='dollars')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
